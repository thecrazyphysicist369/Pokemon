{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-to-image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thecrazyphysicist369/Text-to-Image/blob/main/text_to_image_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAg_I2BUa2Gp",
        "outputId": "a6888c51-942e-4dcb-d12f-a7627be3a6db"
      },
      "source": [
        "! git clone https://github.com/thecrazyphysicist369/keras-text-to-image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-text-to-image'...\n",
            "remote: Enumerating objects: 473, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 473 (delta 3), reused 0 (delta 0), pack-reused 461\u001b[K\n",
            "Receiving objects: 100% (473/473), 11.17 MiB | 40.99 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOVJbGeuozwa"
      },
      "source": [
        "#DC GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cg6kQGD03z5"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/keras-text-to-image/keras_text_to_image/library/utility')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6OGfIEWsT27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "f2e88a6e-eddf-4e27-ae31-8646ceefc240"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Reshape, concatenate\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from image_utils import combine_normalized_images, img_from_normalized_img\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import math\n",
        "from glove_loader import GloveModel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-aafcceed6c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloveModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/keras-text-to-image/keras_text_to_image/library/utility/glove_loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/keras-text-to-image/keras_text_to_image/library/utility'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdownload_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute 'insert'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G08w2Y7ba1J"
      },
      "source": [
        "class DCGan(object):\n",
        "    model_name = 'dc-gan'\n",
        "\n",
        "    def __init__(self):\n",
        "        K.set_image_data_format('channels_last')\n",
        "        self.generator = None\n",
        "        self.discriminator = None\n",
        "        self.model = None\n",
        "        self.img_width = 7\n",
        "        self.img_height = 7\n",
        "        self.img_channels = 1\n",
        "        self.random_input_dim = 100\n",
        "        self.text_input_dim = 100\n",
        "        self.config = None\n",
        "        self.glove_source_dir_path = './very_large_data'\n",
        "        self.glove_model = GloveModel()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_config_file_path(model_dir_path):\n",
        "        return os.path.join(model_dir_path, DCGan.model_name + '-config.npy')\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weight_file_path(model_dir_path, model_type):\n",
        "        return os.path.join(model_dir_path, DCGan.model_name + '-' + model_type + '-weights.h5')\n",
        "\n",
        "    def create_model(self):\n",
        "        init_img_width = self.img_width // 4\n",
        "        init_img_height = self.img_height // 4\n",
        "\n",
        "        random_input = Input(shape=(self.random_input_dim,))\n",
        "        text_input1 = Input(shape=(self.text_input_dim,))\n",
        "        random_dense = Dense(1024)(random_input)\n",
        "        text_layer1 = Dense(1024)(text_input1)\n",
        "\n",
        "        merged = concatenate([random_dense, text_layer1])\n",
        "        generator_layer = Activation('tanh')(merged)\n",
        "\n",
        "        generator_layer = Dense(128 * init_img_width * init_img_height)(generator_layer)\n",
        "        generator_layer = BatchNormalization()(generator_layer)\n",
        "        generator_layer = Activation('tanh')(generator_layer)\n",
        "        generator_layer = Reshape((init_img_width, init_img_height, 128),\n",
        "                                  input_shape=(128 * init_img_width * init_img_height,))(generator_layer)\n",
        "        generator_layer = UpSampling2D(size=(2, 2))(generator_layer)\n",
        "        generator_layer = Conv2D(64, kernel_size=5, padding='same')(generator_layer)\n",
        "        generator_layer = Activation('tanh')(generator_layer)\n",
        "        generator_layer = UpSampling2D(size=(2, 2))(generator_layer)\n",
        "        generator_layer = Conv2D(self.img_channels, kernel_size=5, padding='same')(generator_layer)\n",
        "        generator_output = Activation('tanh')(generator_layer)\n",
        "\n",
        "        self.generator = Model([random_input, text_input1], generator_output)\n",
        "\n",
        "        self.generator.compile(loss='mean_squared_error', optimizer=\"SGD\")\n",
        "\n",
        "        print('generator: ', self.generator.summary())\n",
        "\n",
        "        text_input2 = Input(shape=(self.text_input_dim,))\n",
        "        text_layer2 = Dense(1024)(text_input2)\n",
        "\n",
        "        img_input2 = Input(shape=(self.img_width, self.img_height, self.img_channels))\n",
        "        img_layer2 = Conv2D(64, kernel_size=(5, 5), padding='same')(\n",
        "            img_input2)\n",
        "        img_layer2 = Activation('tanh')(img_layer2)\n",
        "        img_layer2 = MaxPooling2D(pool_size=(2, 2))(img_layer2)\n",
        "        img_layer2 = Conv2D(128, kernel_size=5)(img_layer2)\n",
        "        img_layer2 = Activation('tanh')(img_layer2)\n",
        "        img_layer2 = MaxPooling2D(pool_size=(2, 2))(img_layer2)\n",
        "        img_layer2 = Flatten()(img_layer2)\n",
        "        img_layer2 = Dense(1024)(img_layer2)\n",
        "\n",
        "        merged = concatenate([img_layer2, text_layer2])\n",
        "\n",
        "        discriminator_layer = Activation('tanh')(merged)\n",
        "        discriminator_layer = Dense(1)(discriminator_layer)\n",
        "        discriminator_output = Activation('sigmoid')(discriminator_layer)\n",
        "\n",
        "        self.discriminator = Model([img_input2, text_input2], discriminator_output)\n",
        "\n",
        "        d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
        "\n",
        "        print('discriminator: ', self.discriminator.summary())\n",
        "\n",
        "        model_output = self.discriminator([self.generator.output, text_input1])\n",
        "\n",
        "        self.model = Model([random_input, text_input1], model_output)\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
        "\n",
        "        print('generator-discriminator: ', self.model.summary())\n",
        "\n",
        "    def load_model(self, model_dir_path):\n",
        "        config_file_path = DCGan.get_config_file_path(model_dir_path)\n",
        "        self.config = np.load(config_file_path).item()\n",
        "        self.img_width = self.config['img_width']\n",
        "        self.img_height = self.config['img_height']\n",
        "        self.img_channels = self.config['img_channels']\n",
        "        self.random_input_dim = self.config['random_input_dim']\n",
        "        self.text_input_dim = self.config['text_input_dim']\n",
        "        self.glove_source_dir_path = self.config['glove_source_dir_path']\n",
        "        self.create_model()\n",
        "        self.glove_model.load(self.glove_source_dir_path, embedding_dim=self.text_input_dim)\n",
        "        self.generator.load_weights(DCGan.get_weight_file_path(model_dir_path, 'generator'))\n",
        "        self.discriminator.load_weights(DCGan.get_weight_file_path(model_dir_path, 'discriminator'))\n",
        "\n",
        "    def fit(self, model_dir_path, image_label_pairs, epochs=None, batch_size=None, snapshot_dir_path=None,\n",
        "            snapshot_interval=None):\n",
        "        if epochs is None:\n",
        "            epochs = 100\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = 128\n",
        "\n",
        "        if snapshot_interval is None:\n",
        "            snapshot_interval = 20\n",
        "\n",
        "        self.config = dict()\n",
        "        self.config['img_width'] = self.img_width\n",
        "        self.config['img_height'] = self.img_height\n",
        "        self.config['random_input_dim'] = self.random_input_dim\n",
        "        self.config['text_input_dim'] = self.text_input_dim\n",
        "        self.config['img_channels'] = self.img_channels\n",
        "        self.config['glove_source_dir_path'] = self.glove_source_dir_path\n",
        "\n",
        "        self.glove_model.load(data_dir_path=self.glove_source_dir_path, embedding_dim=self.text_input_dim)\n",
        "\n",
        "        config_file_path = DCGan.get_config_file_path(model_dir_path)\n",
        "\n",
        "        np.save(config_file_path, self.config)\n",
        "        noise = np.zeros((batch_size, self.random_input_dim))\n",
        "        text_batch = np.zeros((batch_size, self.text_input_dim))\n",
        "\n",
        "        self.create_model()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Epoch is\", epoch)\n",
        "            batch_count = int(image_label_pairs.shape[0] / batch_size)\n",
        "            print(\"Number of batches\", batch_count)\n",
        "            for batch_index in range(batch_count):\n",
        "                # Step 1: train the discriminator\n",
        "\n",
        "                image_label_pair_batch = image_label_pairs[batch_index * batch_size:(batch_index + 1) * batch_size]\n",
        "\n",
        "                image_batch = []\n",
        "                for index in range(batch_size):\n",
        "                    image_label_pair = image_label_pair_batch[index]\n",
        "                    normalized_img = image_label_pair[0]\n",
        "                    text = image_label_pair[1]\n",
        "                    image_batch.append(normalized_img)\n",
        "                    text_batch[index, :] = self.glove_model.encode_doc(text, self.text_input_dim)\n",
        "                    noise[index, :] = np.random.uniform(-1, 1, self.random_input_dim)\n",
        "\n",
        "                image_batch = np.array(image_batch)\n",
        "\n",
        "                # image_batch = np.transpose(image_batch, (0, 2, 3, 1))\n",
        "                generated_images = self.generator.predict([noise, text_batch], verbose=0)\n",
        "\n",
        "                if (epoch * batch_size + batch_index) % snapshot_interval == 0 and snapshot_dir_path is not None:\n",
        "                    self.save_snapshots(generated_images, snapshot_dir_path=snapshot_dir_path,\n",
        "                                        epoch=epoch, batch_index=batch_index)\n",
        "\n",
        "                self.discriminator.trainable = True\n",
        "                d_loss = self.discriminator.train_on_batch([np.concatenate((image_batch, generated_images)),\n",
        "                                                            np.concatenate((text_batch, text_batch))],\n",
        "                                                           np.array([1] * batch_size + [0] * batch_size))\n",
        "                print(\"Epoch %d batch %d d_loss : %f\" % (epoch, batch_index, d_loss))\n",
        "\n",
        "                # Step 2: train the generator\n",
        "                for index in range(batch_size):\n",
        "                    noise[index, :] = np.random.uniform(-1, 1, self.random_input_dim)\n",
        "                self.discriminator.trainable = False\n",
        "                g_loss = self.model.train_on_batch([noise, text_batch], np.array([1] * batch_size))\n",
        "\n",
        "                print(\"Epoch %d batch %d g_loss : %f\" % (epoch, batch_index, g_loss))\n",
        "                if (epoch * batch_size + batch_index) % 10 == 9:\n",
        "                    self.generator.save_weights(DCGan.get_weight_file_path(model_dir_path, 'generator'), True)\n",
        "                    self.discriminator.save_weights(DCGan.get_weight_file_path(model_dir_path, 'discriminator'), True)\n",
        "\n",
        "        self.generator.save_weights(DCGan.get_weight_file_path(model_dir_path, 'generator'), True)\n",
        "        self.discriminator.save_weights(DCGan.get_weight_file_path(model_dir_path, 'discriminator'), True)\n",
        "\n",
        "    def generate_image_from_text(self, text):\n",
        "        noise = np.zeros(shape=(1, self.random_input_dim))\n",
        "        encoded_text = np.zeros(shape=(1, self.text_input_dim))\n",
        "        encoded_text[0, :] = self.glove_model.encode_doc(text)\n",
        "        noise[0, :] = np.random.uniform(-1, 1, self.random_input_dim)\n",
        "        generated_images = self.generator.predict([noise, encoded_text], verbose=0)\n",
        "        generated_image = generated_images[0]\n",
        "        generated_image = generated_image * 127.5 + 127.5\n",
        "        return Image.fromarray(generated_image.astype(np.uint8))\n",
        "\n",
        "    def save_snapshots(self, generated_images, snapshot_dir_path, epoch, batch_index):\n",
        "        image = combine_normalized_images(generated_images)\n",
        "        img_from_normalized_img(image).save(\n",
        "            os.path.join(snapshot_dir_path, DCGan.model_name + '-' + str(epoch) + \"-\" + str(batch_index) + \".png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NO70wlLpsuj"
      },
      "source": [
        "#DC GAN Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH5YFmi8_4V8"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "sys.path.insert(1,'/content/keras-text-to-image/keras_text_to_image/library')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "ovq5AEW3pvFS",
        "outputId": "e0ec7ea4-33c2-4fb3-d312-95bfb3739bff"
      },
      "source": [
        "def main():\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    '''\n",
        "    current_dir = os.path.dirname(__file__)\n",
        "    sys.path.append(os.path.join(current_dir, '..'))\n",
        "    current_dir = current_dir if current_dir is not '' else '.'\n",
        "\n",
        "    img_dir_path = current_dir + '/data/pokemon/img'\n",
        "    txt_dir_path = current_dir + '/data/pokemon/txt'\n",
        "    model_dir_path = current_dir + '/models'\n",
        "    '''\n",
        "\n",
        "    img_dir_path = '/content/keras-text-to-image/demo/data/pokemon/img'\n",
        "    txt_dir_path = '/content/keras-text-to-image/demo/data/pokemon/txt'\n",
        "    model_dir_path = '/content/keras-text-to-image/demo/models'\n",
        "\n",
        "\n",
        "    img_width = 32\n",
        "    img_height = 32\n",
        "    img_channels = 3\n",
        "\n",
        "    \n",
        "    #from dcgan import DCGan\n",
        "    from img_cap_loader import load_normalized_img_and_its_text\n",
        "\n",
        "    image_label_pairs = load_normalized_img_and_its_text(img_dir_path, \n",
        "                                                         txt_dir_path, \n",
        "                                                         img_width=img_width, \n",
        "                                                         img_height=img_height)\n",
        "\n",
        "    shuffle(image_label_pairs)\n",
        "\n",
        "    gan = DCGan()\n",
        "    gan.img_width = img_width\n",
        "    gan.img_height = img_height\n",
        "    gan.img_channels = img_channels\n",
        "    gan.random_input_dim = 200\n",
        "    gan.glove_source_dir_path = './very_large_data'\n",
        "\n",
        "    batch_size = 16\n",
        "    epochs = 1000\n",
        "    gan.fit(model_dir_path=model_dir_path, image_label_pairs=image_label_pairs,\n",
        "            snapshot_dir_path='/content/keras-text-to-image/demo/data/snapshots',\n",
        "            snapshot_interval=100,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-text-to-image/keras_text_to_image/library/utility/img_cap_loader.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7fd4cb6d896c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-7fd4cb6d896c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_label_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-047ef656f19f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# self.glove_source_dir_path = './very_large_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGloveModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GloveModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypaz_Nw69QkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ef91d2-5e11-4729-a9b6-c9fa3709618e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras-text-to-image  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZZIEFAXpl6T"
      },
      "source": [
        "#DC GAN Generate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_24ChCoprTG"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def main():\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    current_dir = os.path.dirname(__file__)\n",
        "    sys.path.append(os.path.join(current_dir, '..'))\n",
        "    current_dir = current_dir if current_dir is not '' else '.'\n",
        "\n",
        "    img_dir_path = current_dir + '/data/pokemon/img'\n",
        "    txt_dir_path = current_dir + '/data/pokemon/txt'\n",
        "    model_dir_path = current_dir + '/models'\n",
        "\n",
        "    img_width = 32\n",
        "    img_height = 32\n",
        "\n",
        "    from keras_text_to_image.library.dcgan import DCGan\n",
        "    from keras_text_to_image.library.utility.image_utils import img_from_normalized_img\n",
        "    from keras_text_to_image.library.utility.img_cap_loader import load_normalized_img_and_its_text\n",
        "\n",
        "    image_label_pairs = load_normalized_img_and_its_text(img_dir_path, txt_dir_path, img_width=img_width, img_height=img_height)\n",
        "\n",
        "    shuffle(image_label_pairs)\n",
        "\n",
        "    gan = DCGan()\n",
        "    gan.load_model(model_dir_path)\n",
        "\n",
        "    for i in range(10):\n",
        "        image_label_pair = image_label_pairs[i]\n",
        "        normalized_image = image_label_pair[0]\n",
        "        text = image_label_pair[1]\n",
        "\n",
        "        image = img_from_normalized_img(normalized_image)\n",
        "        image.save(current_dir + '/data/outputs/' + DCGan.model_name + '-generated-' + str(i) + '-0.png')\n",
        "        for j in range(3):\n",
        "            generated_image = gan.generate_image_from_text(text)\n",
        "            generated_image.save(current_dir + '/data/outputs/' + DCGan.model_name + '-generated-' + str(i) + '-' + str(j) + '.png')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}